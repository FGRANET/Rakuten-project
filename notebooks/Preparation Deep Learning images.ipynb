{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73868d46",
   "metadata": {},
   "source": [
    "# Préparation Deep Learning Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "795f22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ddfef",
   "metadata": {},
   "source": [
    "# Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "742ed35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def path_file(i,df):\n",
    "    ### fonction pour aller chercher le chemin d'accès d'un photo de rang i dans le dateframe train_image###\n",
    "    img = df['File'].iloc[i]\n",
    "    path_file = f\"C:\\\\Users\\\\xavie\\\\Projet Rakuten\\\\images\\\\images\\\\image_train\\\\{img}\"\n",
    "    #path_file = f\"C:\\\\Users\\\\Frederic\\\\Projet Rakuten\\\\images\\\\images\\\\image_train\\\\{img}\"\n",
    "    \n",
    "    return path_file\n",
    "\n",
    "def supprimer_bords(img, threshold_black=10, threshold_white=245):\n",
    "    ### fonction pour supprimer les bords noirs ou blances des photos ###    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Pour les bords noirs\n",
    "    _, thresh_black = cv2.threshold(gray, threshold_black, 255, cv2.THRESH_BINARY)\n",
    "    contours_black, _ = cv2.findContours(thresh_black, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if len(contours_black) > 0:\n",
    "        cnt_black = max(contours_black, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(cnt_black)\n",
    "        img = img[y:y+h, x:x+w]\n",
    "\n",
    "    # Pour les bords blancs\n",
    "    _, thresh_white = cv2.threshold(gray, threshold_white, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours_white, _ = cv2.findContours(thresh_white, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours_white) > 0:\n",
    "        cnt_white = max(contours_white, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(cnt_white)\n",
    "        img = img[y:y+h, x:x+w]\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def traiter_images(filepath, IMG_SIZE, output_directory, echantillon=None):\n",
    "    ### Fonction pour traiter les images du dataset et créer des repertoires pour le DL\n",
    "    \n",
    "    data = []  # Pour stocker les données (images traitées)\n",
    "    \n",
    "    # Utiliser glob pour obtenir la liste des fichiers correspondant au motif\n",
    "    image_files = glob.glob(filepath + '/*.jpg')  \n",
    "     \n",
    "    # Initialiser un compteur pour suivre le nombre d'images traitées\n",
    "    images_traitees = 0  \n",
    "    \n",
    "    # Liste pour stocker les exceptions\n",
    "    exceptions = []  \n",
    "            \n",
    "    for image_file in image_files:\n",
    "        if (echantillon is not None) and (images_traitees >= echantillon):\n",
    "            break\n",
    "        try:\n",
    "            img = cv2.imread(image_file)\n",
    "                        \n",
    "            #traiter l'image\n",
    "            image_resultat = supprimer_bords(img)         \n",
    "            image_resultat = cv2.resize(image_resultat, (IMG_SIZE,IMG_SIZE))\n",
    "\n",
    "            if image_resultat is not None:\n",
    "                image_name = image_file.split('\\\\')[-1]  #obtenir le nom de l'image\n",
    "                                                 # Enregistrer l'image résultante dans le dossier de sortie            \n",
    "                output_path = os.path.join(output_directory, image_name)\n",
    "                cv2.imwrite(output_path, image_resultat)\n",
    "            print(output_path)  \n",
    "            \n",
    "            # Utiliser des opérations de chaîne pour extraire les informations nécessaires\n",
    "            parts = image_name.split('_')\n",
    "            \n",
    "            image_id = parts[1]\n",
    "            product_id = parts[3].split('.')[0]\n",
    "            \n",
    "            # Obtenez la largeur, la hauteur et la taille en bits de l'image\n",
    "            height, width, size = image_resultat.shape[0], image_resultat.shape[1], image_resultat.nbytes*8\n",
    "            \n",
    "            # Créez un dictionnaire avec les informations extraites\n",
    "            dict_image = {'File':image_name, 'image_id':image_id, 'product_id':product_id,'Height':height,\n",
    "                        'Width': width, 'Size in Bits': size}\n",
    "           \n",
    "            # Ajouter le dictionnaire à la liste de données\n",
    "            data.append(dict_image)\n",
    "\n",
    "            # Incrémenter le compteur d'images traitées\n",
    "            images_traitees += 1\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            exceptions.append(f\"Image {image_name} introuvable.\")\n",
    "        except Exception as e:\n",
    "             exceptions.append(f\"Erreur lors du traitement de l'image {image_file}: {str(e)}\")\n",
    "\n",
    "         \n",
    "       # Affichez les exceptions à la fin\n",
    "    if exceptions:\n",
    "        print(\"\\nExceptions rencontrées :\")\n",
    "    for exception in exceptions:\n",
    "        print(exception)\n",
    "    else:\n",
    "        print(\"\\nAucune exception rencontrée.\")\n",
    "\n",
    "    # Convertir la liste de données en un DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13ed1a",
   "metadata": {},
   "source": [
    "# Traitement des images et création du Fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2988efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aucune exception rencontrée.\n"
     ]
    }
   ],
   "source": [
    "# Réglage des paramètres\n",
    "\n",
    "IMG_SIZE= 224\n",
    "path_file_images_a_traiter = \"C:\\\\Users\\\\xavier\\\\Projet Rakuten\\\\images\\\\images\\\\image_train\"\n",
    "path_file_images_traitees = \"C:\\\\Users\\\\xavier\\\\Projet Rakuten\\\\images\\\\images\\\\images_traitees\"\n",
    "\n",
    "df_train_image=traiter_images(filepath=path_file_images_a_traiter, IMG_SIZE = IMG_SIZE,\n",
    "                              output_directory = path_file_images_traitees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846369c8",
   "metadata": {},
   "source": [
    "## Exploration sur les images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bcdf8390",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'df_train_image.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_train_image \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdf_train_image.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#fonction pour aller chercher le chemin d'accès d'un photo de rang i dans les images d'entrainement\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpath_file_2\u001b[39m(i):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:579\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    577\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'df_train_image.joblib'"
     ]
    }
   ],
   "source": [
    "df_train_image = load('df_train_image.joblib')\n",
    "\n",
    "#fonction pour aller chercher le chemin d'accès d'un photo de rang i dans les images d'entrainement\n",
    "def path_file_2(i):\n",
    "    img = df_train_image['File'].iloc[i]\n",
    "    path_file = f\"C:\\\\Users\\Frederic\\Projet Rakuten\\images\\images\\image_train\\{img}\"\n",
    "    return path_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18033d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Affichage 4 images du dataset au hasard\n",
    "images=[]\n",
    "categories=[]\n",
    "#Representation graphique\n",
    "for k in np.random.choice(np.arange(0, len(df_train_image)), size = 4):\n",
    "        lien = df_train_image['File'].iloc[k]\n",
    "        path_file = f\"C:\\\\Users\\Frederic\\Projet Rakuten\\images\\images\\image_train\\{lien}\"\n",
    "        img_color = cv2.imread(path_file)    \n",
    "        images.append(img_color)\n",
    "        categories.append(df_train_image['categorie'].iloc[k])\n",
    "\n",
    "# Display some augmented samples\n",
    "plt.figure(figsize=(9,9))\n",
    "for count, value in enumerate(images):\n",
    "    plt.subplot(2,2,count+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(value+1)\n",
    "    #plt.xticks([])\n",
    "    #plt.yticks([])\n",
    "    #plt.tight_layout()\n",
    "    plt.title('Catégorie: ' + str(categories[count]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd652abb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m categories\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#Representation graphique\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_train_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      8\u001b[0m         lien \u001b[38;5;241m=\u001b[39m df_train_image[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[k]\n\u001b[0;32m      9\u001b[0m         path_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFrederic\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProjet Rakuten\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimage_train\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlien\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mmtrand.pyx:915\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "# Autre option\n",
    "# Affichage 4 images du dataset au hasard\n",
    "images=[]\n",
    "categories=[]\n",
    "\n",
    "#Representation graphique\n",
    "for k in np.random.choice(np.arange(0, len(df_train_image)), size = 4):\n",
    "        lien = df_train_image['File'].iloc[k]\n",
    "        path_file = f\"C:\\\\Users\\Frederic\\Projet Rakuten\\images\\images\\image_train\\{lien}\"\n",
    "        img_color = cv2.imread(path_file)    \n",
    "        images.append(img_color)\n",
    "        categories.append(df_train_image['categorie'].iloc[k])\n",
    "\n",
    "for j in [1,2,3,4]:\n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    ax = fig.add_subplot(2,2,j)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(images[j-1]) #pour lire  mon_image \n",
    "    #Modification du titre de la figure\n",
    "    plt.title('Catégorie: ' + str(categories[j-1]))\n",
    "    plt.show();\n",
    "    \n",
    "\n",
    "# Affichage 4 images du dataset au hasard pour une catégorie \n",
    "categorie_choisie = 'livres'\n",
    "df_categorie_choisie = df_train_image.loc[df_train_image[\"categorie\"] == categorie_choisie]\n",
    "\n",
    "images=[]\n",
    "\n",
    "#Representation graphique\n",
    "j= 1  \n",
    "for k in np.random.choice(np.arange(0, len(df_categorie_choisie)), size = 4):\n",
    "        lien = df_categorie_choisie['File'].iloc[k]\n",
    "        path_file = f\"C:\\\\Users\\Frederic\\Projet Rakuten\\images\\images\\image_train\\{lien}\"\n",
    "        img_color = cv2.imread(path_file)    \n",
    "        images.append(img_color)\n",
    "\n",
    "for j in [1,2,3,4]:\n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    ax = fig.add_subplot(2,2,j)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(images[j-1]) #pour lire  mon_image \n",
    "    #Modification du titre de la figure\n",
    "    plt.title('Catégorie: ' + str(categorie_choisie))\n",
    "    plt.show();\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af445f",
   "metadata": {},
   "source": [
    "# Chargement du DataFrame et Visualisation du traitement sur des images aléatoires du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2709905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du DataFrame\n",
    "df_train_image = joblib.load(\"df_train_image.joblib\") # Chargez votre propre DataFrame\n",
    "\n",
    "j = 0\n",
    "for i in np.random.choice(np.arange(0, len(df_train_image)), size = 3):\n",
    "    j = j + 1\n",
    "    img = cv2.imread(path_file(i,df_train_image))\n",
    "          \n",
    "    fig = plt.figure(figsize = (5,5))\n",
    "    ax = fig.add_subplot(3,3,j)\n",
    "           \n",
    "    # Suppression des axes\n",
    "    ax.set_axis_off()\n",
    "    # Affichage de la figure \n",
    "    ax.imshow(img) #pour lire l'image non transformée\n",
    "    # Titre de la figure\n",
    "    plt.title('photo avant transformation')\n",
    "    plt.show();  \n",
    "    \n",
    "    j = j + 1 \n",
    "    cropped_image = supprimer_bords(i)\n",
    "    \n",
    "    fig = plt.figure(figsize = (5,5))\n",
    "    ax = fig.add_subplot(3,3,j)\n",
    "      \n",
    "    # Suppression des axes\n",
    "    ax.set_axis_off()\n",
    "    # Affichage de la figure \n",
    "    ax.imshow(cropped_image) #pour lire l'image transformée\n",
    "    #Titre de la figure\n",
    "    plt.title('photo après transformation')\n",
    "    plt.show();   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be995cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51a8a4d6",
   "metadata": {},
   "source": [
    "# Fusion du DataSet Image avec X_train et mise en forme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08232baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84916, 5)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'productid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'productid'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproductid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproductid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimageid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimageid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m df_train_image[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproductid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train_image\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproductid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m df_train_image[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimageid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_train_image[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimageid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#fusion des df\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'productid'"
     ]
    }
   ],
   "source": [
    "#Chargement et assemblage des autres Datasets\n",
    "\n",
    "# PC perso\n",
    "#y_train = pd.read_csv('Y_train_CVw08PX.csv',index_col=0)\n",
    "#X_train_update = pd.read_csv(\"C:\\\\Users\\Frederic\\Projet Rakuten/X_train_update.csv\",index_col=0)\n",
    "\n",
    "# PC portable\n",
    "path_X_train = \"C:\\\\Users\\\\xavie\\\\Documents\\\\GitHub\\\\AVR23_CDS_Rakuten\\\\data\\\\X_train_update.csv\"\n",
    "path_y_train = \"C:\\\\Users\\\\xavie\\\\Documents\\\\GitHub\\\\AVR23_CDS_Rakuten\\\\data\\\\Y_train_CVw08PX.csv\"\n",
    "\n",
    "X_train_update = pd.read_csv(path_X_train,index_col=0)   \n",
    "y_train = pd.read_csv(path_y_train,index_col=0)\n",
    "\n",
    "df = pd.concat([X_train_update,y_train], axis=1)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "#Nommage de la cible\n",
    "dictionnaire_cat={\"10\" : \"livres\", \"40\" : \"jeux video\", \"50\" : \"accessoires jeux video\", \"60\" : \"console jeux video\", \"1140\" : \"goodies\", \"1160\" : \"cartes à collectionner\" ,\n",
    "                  \"1180\":\"jeux de rôle\", \"1280\" : \"jouet\", \"1281\": \"jeux de societe\", \"1300\" : \"modélisme\", \"1301\":\"activité/jeux ext\", \"1302\" : \"activité plein air\",\n",
    "                  \"1320\": \"puericulture\" , \"1560\" : \"equipement ameublement\", \"1920\": \"accessoire literie\", \"1940\" : \"confiserie\", \"2060\" : \"décoration\", \"2220\" : \"accessoire animaux\",\n",
    "                  \"2280\" : \"magazine\", \"2403\": \"revue/album/serie\", \"2462\" : \"lot jeux video\" ,\"2522\" : \"fourniture bureau\", \"2582\" : \"equipement ext\", \"2583\" : \"accessoire piscine\",\n",
    "                  \"2585\" : \"accessoire entretien ext\", \"2705\": \"livre documentaire\", \"2905\" : \"jeux vidéo dématérialisés\"}\n",
    "\n",
    "# Convertir la colonne \"prdtypecode\" en chaîne de caractères\n",
    "df[\"prdtypecode\"] = df[\"prdtypecode\"].astype(\"str\")\n",
    "# Créer une nouvelle colonne \"categorie\" en utilisant le dictionnaire de catégories\n",
    "df[\"categorie\"] = df[\"prdtypecode\"].map(dictionnaire_cat)\n",
    "\n",
    "df_train_image = df_train_image.rename(columns={'product_id':'productid','image_id':'imageid'})\n",
    "\n",
    "#retypage en str des colonnes   \n",
    "df['productid'] = df['productid'].astype(\"str\")\n",
    "df[\"imageid\"] = df[\"imageid\"].astype(\"str\")\n",
    "df_train_image['productid'] = df_train_image['productid'].astype(\"str\")\n",
    "df_train_image['imageid'] = df_train_image['imageid'].astype(\"str\")\n",
    "\n",
    "#fusion des df\n",
    "df_train_image = df_train_image.merge(right = df, how = 'inner', on = [\"productid\",\"imageid\"])\n",
    "to_drop=['designation','description']\n",
    "df_train_image.drop(to_drop,axis=1) \n",
    "dump(df_train_image, 'df_train_image.joblib')\n",
    "\n",
    "# creation ['df_train_image.joblib']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erreur à gérer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865263d6",
   "metadata": {},
   "source": [
    "# Rangement des images modifiées  par Classe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98793d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données\n",
    "X_train,X_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db32671c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prdtypecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'prdtypecode'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mxavie\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mProjet Rakuten\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mimages_traitees_classe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Créez des sous-répertoires pour chaque classe\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_code \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprdtypecode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique():\n\u001b[0;32m     15\u001b[0m     class_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;28mstr\u001b[39m(class_code))\n\u001b[0;32m     16\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(class_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'prdtypecode'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "df = df_train_image\n",
    "\n",
    "# Chemin vers le répertoire contenant les images à classer\n",
    "image_dir = \"C:\\\\Users\\\\xavie\\\\Projet Rakuten\\\\images\\\\images\\\\images_traitees\"\n",
    "    \n",
    "# Chemin vers le répertoire de sortie où les images classées seront déplacées\n",
    "output_dir = \"C:\\\\Users\\\\xavie\\\\Projet Rakuten\\\\images\\\\images\\\\images_traitees_classe\"\n",
    "\n",
    "# Créez des sous-répertoires pour chaque classe\n",
    "for class_code in df['prdtypecode'].unique():\n",
    "    class_dir = os.path.join(output_dir, str(class_code))\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "# Train\n",
    "# Liste pour stocker les exceptions\n",
    "exceptions = []\n",
    "# Parcourez le DataFrame et déplacez les images vers les sous-répertoires appropriés\n",
    "for index, row in X_train.iterrows():\n",
    "    image_name = row['File'].split('\\\\')[-1]\n",
    "    productid = row['productid']\n",
    "    imageid= row['imageid']\n",
    "    class_code = row['prdtypecode']\n",
    "\n",
    "    # Chemin complet de l'image source\n",
    "    source_image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "    # Chemin du répertoire de destination pour cette classe\n",
    "    class_output_dir = os.path.join(output_dir_train, str(class_code))\n",
    "\n",
    "    # Chemin complet de l'image de destination\n",
    "    destination_image_path = os.path.join(class_output_dir, image_name)\n",
    "\n",
    "    try:\n",
    "        # Déplacez l'image vers le répertoire de destination\n",
    "        shutil.move(source_image_path, destination_image_path)\n",
    "        print(f\"Image {image_name} classée avec succès.\")\n",
    "    except FileNotFoundError:\n",
    "        exceptions.append(f\"Image {image_name} introuvable.\")\n",
    "    except Exception as e:\n",
    "        exceptions.append(f\"Erreur lors du déplacement de l'image {image_name}: {str(e)}\")\n",
    "\n",
    "# Affichez les exceptions à la fin\n",
    "if exceptions:\n",
    "    print(\"\\nExceptions rencontrées :\")\n",
    "    for exception in exceptions:\n",
    "        print(exception)\n",
    "else:\n",
    "    print(\"\\nAucune exception rencontrée.\")\n",
    "\n",
    "print(\"\\nClassification automatique terminée.\")\n",
    "\n",
    "# Test\n",
    "# Liste pour stocker les exceptions\n",
    "exceptions = []\n",
    "# Parcourez le DataFrame et déplacez les images vers les sous-répertoires appropriés\n",
    "\n",
    "for index, row in X_test.iterrows():\n",
    "    image_name = row['File'].split('\\\\')[-1]\n",
    "    productid = row['productid']\n",
    "    imageid= row['imageid']\n",
    "    class_code = row['prdtypecode']\n",
    "\n",
    "    # Chemin complet de l'image source\n",
    "    source_image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "    # Chemin du répertoire de destination pour cette classe\n",
    "    class_output_dir = os.path.join(output_dir_test, str(class_code))\n",
    "\n",
    "    # Chemin complet de l'image de destination\n",
    "    destination_image_path = os.path.join(class_output_dir, image_name)\n",
    "\n",
    "    try:\n",
    "        # Déplacez l'image vers le répertoire de destination\n",
    "        shutil.move(source_image_path, destination_image_path)\n",
    "        print(f\"Image {image_name} classée avec succès.\")\n",
    "    except FileNotFoundError:\n",
    "        exceptions.append(f\"Image {image_name} introuvable.\")\n",
    "    except Exception as e:\n",
    "        exceptions.append(f\"Erreur lors du déplacement de l'image {image_name}: {str(e)}\")\n",
    "\n",
    "# Affichez les exceptions à la fin\n",
    "if exceptions:\n",
    "    print(\"\\nExceptions rencontrées :\")\n",
    "    for exception in exceptions:\n",
    "        print(exception)\n",
    "else:\n",
    "    print(\"\\nAucune exception rencontrée.\")\n",
    "\n",
    "print(\"\\nClassification automatique terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909e12c",
   "metadata": {},
   "source": [
    "## Genération d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0503501a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\xavie\\\\Documents\\\\GitHub\\\\AVR23_CDS_Rakuten\\\\data\\\\df_train_image.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mxavie\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mAVR23_CDS_Rakuten\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdf_train_image.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Chemin Frédéric Hanène\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#Chemin Franck\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#CHemin karima\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df_train_image \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# fonction pour le chemin dans le nouveau dataaset\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpath_file_2\u001b[39m(i):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:579\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    577\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\xavie\\\\Documents\\\\GitHub\\\\AVR23_CDS_Rakuten\\\\data\\\\df_train_image.joblib'"
     ]
    }
   ],
   "source": [
    "# Chargement du DataFrame\n",
    "\n",
    "#Chemin Frédéric\n",
    "path = \"C:\\\\Users\\\\xavie\\\\Documents\\\\GitHub\\\\AVR23_CDS_Rakuten\\\\data\\\\df_train_image.joblib\" \n",
    "#Chemin Frédéric Hanène\n",
    "#Chemin Franck\n",
    "#CHemin karima\n",
    "\n",
    "df_train_image = joblib.load(path) \n",
    "\n",
    "# fonction pour le chemin dans le nouveau dataaset\n",
    "def path_file_2(i):\n",
    "    classe= str(df_train_image['prdtypecode'].iloc[i])\n",
    "    print(classe)\n",
    "    img = df_train_image['File'].iloc[i]\n",
    "    \n",
    "    #Chemin Frédéric\n",
    "    path_file = f\"C:\\\\Users\\\\Frederic\\\\Projet Rakuten\\\\images\\\\images\\\\train\\\\{classe}\\{img}\"\n",
    "    \n",
    "    print(path_file)\n",
    "    return path_file\n",
    "\n",
    "# Visualisation de la génération d'images pour une image du Dataset\n",
    "\n",
    "numero_image= 266 # a renseigner\n",
    "\n",
    "# Chargement de l'image et transformation en matrice et expansion des dimensions\n",
    "img = load_img(path_file_2(numero_image))\n",
    "img = img_to_array(img)\n",
    "img1 = np.expand_dims(img, axis=0)\n",
    " \n",
    "# Définir les paramètres du générateur d'images aléatoires\n",
    "generator = ImageDataGenerator(\n",
    "    #rescale=1.0 / 255.0, #rescale\n",
    "    rotation_range=20,  # Angle de rotation aléatoire\n",
    "    width_shift_range=0.2,  # Déplacement horizontal aléatoire\n",
    "    height_shift_range=0.2,  # Déplacement vertical aléatoire\n",
    "    shear_range=0.2,  # Déformation aléatoire\n",
    "    zoom_range=0.2,  # Zoom aléatoire\n",
    "    horizontal_flip=True,  # Retournement horizontal aléatoire\n",
    "    fill_mode='nearest'  # Mode de remplissage pour les nouvelles zones créées\n",
    ")\n",
    "\n",
    "data_generator = generator.flow(img1,batch_size=2)\n",
    " \n",
    "# Visualisation de quelques augmentations d'images\n",
    "plt.figure(figsize=(10,8))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    for x in data_generator:\n",
    "        plt.imshow(x[0]/255.)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6917e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
