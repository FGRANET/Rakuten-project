{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb3f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user transformers==4.26.1 datasets==2.10.1 evaluate==0.4.0 -q \n",
    "!pip install --user --upgrade datasets transformers pyarrow\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb7febc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, Dataset, Features, Array3D\n",
    "from io import BytesIO\n",
    "from transformers import AutoProcessor, ViTFeatureExtractor, ViTForImageClassification, Trainer, TrainingArguments, default_data_collator\n",
    "from typing import Tuple\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d76ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory where our images are saved in folders by category\n",
    "# Charger les données\n",
    "data_path = \"C:/Users/h.chettaoui/Documents/AVR23_CDS_Rakuten/images/images_train\"\n",
    "test_data_path = \"C:/Users/h.chettaoui/Documents/AVR23_CDS_Rakuten/images/images_test\"\n",
    "\n",
    "# The output directory of the processed datasets\n",
    "train_save_path = \"./processed-datasets/train\"\n",
    "val_save_path = \"./processed-datasets/val\"\n",
    "test_save_path = \"./processed-datasets/test\"\n",
    "\n",
    "# Sizes of dataset splits\n",
    "val_size = 0.2\n",
    "test_size = 0.1\n",
    "\n",
    "# Name of model as named in the HuggingFace Hub\n",
    "model_name = \"google/vit-base-patch16-224\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03afbd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e72b769c43f43ada5a7dc91a8b52a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/67931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb2e5023b8943a5a6dc3da2a3b920be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/67931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7a8c61088443c29f8c082b7c818969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c06c75a50742f594d52ac20f1a0d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ec20f79ff94c02b465286ac83d1397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8badf9c8c245979a4a2a7a7a124799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/67931 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imagefolder\", data_dir=data_path, split='train')\n",
    "\n",
    "# Remove from dataset images which are non-RGB (single-channel, grayscale)\n",
    "condition = lambda data: data['image'].mode == 'RGB'\n",
    "dataset = dataset.filter(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e64979b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train, validation, and test sets...\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(dataset, val_size=0.2, test_size=0.1):\n",
    "    \"\"\"\n",
    "    Returns a tuple with three random train, validation, and test subsets by splitting the passed dataset.\n",
    "    Size of the validation and test sets defined as a fraction of 1 with the `val_size` and `test_size` arguments.\n",
    "    \"\"\"\n",
    "    print(\"Splitting dataset into train, validation, and test sets...\")\n",
    "\n",
    "    # Calculate the total size of the validation and test sets\n",
    "    split_size = round(val_size + test_size, 3)\n",
    "\n",
    "    # Split the dataset into train and (val + test) sets\n",
    "    dataset = dataset.train_test_split(shuffle=True, test_size=split_size)\n",
    "\n",
    "    # Calculate the ratio of the test set size to the total of (val + test) set size\n",
    "    split_ratio = round(test_size / (test_size + val_size), 3)\n",
    "\n",
    "    # Split the (val + test) set into validation and test sets\n",
    "    val_test_sets = dataset['test'].train_test_split(shuffle=True, test_size=split_ratio)\n",
    "\n",
    "    # Assign the train, validation, and test sets to variables\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    val_dataset = val_test_sets[\"train\"]\n",
    "    test_dataset = val_test_sets[\"test\"]\n",
    "\n",
    "    # Return the train, validation, and test sets as a tuple\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Split the dataset into train, validation, and test sets using the specified sizes\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20015620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 47551\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e017a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_examples(examples, image_processor):\n",
    "    \"\"\"Processor helper function. Used to process batches of images using the\n",
    "    passed image_processor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples\n",
    "        A batch of image examples.\n",
    "\n",
    "    image_processor\n",
    "        A HuggingFace image processor for the selected model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    examples \n",
    "        A batch of processed image examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Récupérer le lot d'images\n",
    "    images = examples['image']\n",
    "\n",
    "    # Prétraiter les images\n",
    "    inputs = image_processor(images=images)\n",
    "    \n",
    "    # Ajouter les valeurs de pixels prétraitées aux exemples\n",
    "    examples['pixel_values'] = inputs['pixel_values']\n",
    "\n",
    "    # Libérer la mémoire si les images ne sont plus nécessaires\n",
    "    del images\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b6f62c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386ff42492784fe5b59651a050410e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc616792975410994a6ff2faf9dd028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13593 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e484501b26942b2a3aad652ad30c525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_processing(model_name, train_dataset, val_dataset, test_dataset):\n",
    " \n",
    "    \"\"\"\n",
    "    Apply model's image AutoProcessor to transform train, validation, and test subsets.\n",
    "    AutoProcessor effectue le Redimensionnement,Normalisation,onversion en tableau de pixels, Positionnement des patchs...\n",
    "    Returns train, validation, and test datasets with `pixel_values` in torch tensor type.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extend the features\n",
    "    features = train_dataset.features.copy()\n",
    "    features.update({\n",
    "        'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    })\n",
    "\n",
    "    ''' features=\n",
    "    {'image': Image(decode=True, id=None),\n",
    "     'label': ClassLabel(names=['10', '1140', '1160', '1180', '1280', '1281', '1300', '1301', '1302', '1320', '1560', '1920', '1940', '2060', '2220', '2280', '2403', '2462', '2522', '2582', '2583', '2585', '2705', '2905', '40', '50', '60'], id=None),\n",
    "     'pixel_values': Array3D(shape=(3, 224, 224), dtype='float32', id=None)\n",
    "     }\n",
    "    '''\n",
    "    \n",
    "    # Instantiate image_processor\n",
    "    image_processor = AutoProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    datasets = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n",
    "\n",
    "    for dataset_name in [\"train\", \"val\", \"test\"]:\n",
    "        \n",
    "        datasets[dataset_name] = datasets[dataset_name].map(process_examples, batched=True, features=features, fn_kwargs={\"image_processor\": image_processor})\n",
    "       \n",
    "        datasets[dataset_name].set_format('torch', columns=['pixel_values', 'label'])\n",
    "        \n",
    "        datasets[dataset_name] = datasets[dataset_name].remove_columns(\"image\")\n",
    "    \n",
    "    \n",
    "    return datasets[\"train\"], datasets[\"val\"], datasets[\"test\"]\n",
    "\n",
    "\n",
    "# Apply AutoProcessor\n",
    "train_dataset, val_dataset, test_dataset = apply_processing(model_name,train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd99cbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'pixel_values'],\n",
       "    num_rows: 47551\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "941003f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03544df11d44e80b06a4fac4c970313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/47551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f55f5f7cfb941c3a5b210aa5d287b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/13593 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4f31cea5c3445ea89239d6b7b07371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save train, validation and test preprocessed datasets\n",
    "train_dataset.save_to_disk(train_save_path, num_shards=1)\n",
    "\n",
    "val_dataset.save_to_disk(val_save_path, num_shards=1)\n",
    "\n",
    "test_dataset.save_to_disk(test_save_path, num_shards=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8aace17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk(train_save_path)\n",
    "\n",
    "val_dataset = load_from_disk(val_save_path)\n",
    "\n",
    "test_dataset=load_from_disk(test_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9083f41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'pixel_values'],\n",
       "    num_rows: 47551\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41edb615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'pixel_values'],\n",
       "    num_rows: 67\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Travailler avec 1% des données\n",
    "# Specify the percentage of data to keep (1% in this case)\n",
    "\n",
    "subset_percentage = 0.001\n",
    "\n",
    "# Split the datasets into a small subset (1%)\n",
    "\n",
    "train_dataset= train_dataset.train_test_split(shuffle=True, test_size=0.99)[\"train\"]\n",
    "val_dataset= val_dataset.train_test_split(shuffle=True, test_size=0.99)[\"train\"]\n",
    "test_dataset= test_dataset.train_test_split(shuffle=True, test_size=0.99)[\"train\"]\n",
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92b4d18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1348c32dfc84c8ba6dd53f5ec83e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([27]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([27, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_classes = train_dataset.features[\"label\"].num_classes\n",
    "\n",
    "# Download model from model hub : Décomposition de l'image en patchs,Tokenisation des patchs,Position Embeddings,Encodage par les couches Transformer,Sortie de Classification\n",
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=num_classes, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Download feature extractor from hub\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70bf0d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83389b676b9b48ce84e951e1b5c0553a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Compute metrics function for binary classification\n",
    "acc_metric = evaluate.load(\"accuracy\", module_type=\"metric\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predicted_probs, labels = eval_pred\n",
    "    # Accuracy\n",
    "    predicted_labels = np.argmax(predicted_probs, axis=1)\n",
    "    acc = acc_metric.compute(predictions=predicted_labels, references=labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf2505b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the correspondance 'id2label' et 'label2id' to have  class names\n",
    "# Change labels\n",
    "id2label = {key:train_dataset.features[\"label\"].names[index] for index,key in enumerate(model.config.id2label.keys())}\n",
    "label2id = {train_dataset.features[\"label\"].names[index]:value for index,value in enumerate(model.config.label2id.values())}\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f7ad57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./model\"\n",
    "output_data_dir = \"./outputs\"\n",
    "\n",
    "# Total number of training epochs to perform\n",
    "num_train_epochs = 10\n",
    "# The batch size per GPU/TPU core/CPU for training\n",
    "per_device_train_batch_size = 32\n",
    "# The batch size per GPU/TPU core/CPU for evaluation\n",
    "per_device_eval_batch_size = 64\n",
    "# The initial learning rate for AdamW optimizer\n",
    "learning_rate = 2e-5\n",
    "# Number of steps used for a linear warmup from 0 to learning_rate\n",
    "warmup_steps = 500\n",
    "# The weight decay to apply to all layers except all bias and LayerNorm weights in AdamW optimizer\n",
    "weight_decay = 0.01\n",
    "\n",
    "main_metric_for_evaluation = \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "633331cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = model_dir,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "    warmup_steps = warmup_steps,\n",
    "    weight_decay = weight_decay,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    logging_dir = f\"{output_data_dir}/logs\",\n",
    "    learning_rate = float(learning_rate),\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = main_metric_for_evaluation,\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    compute_metrics = compute_metrics,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator = default_data_collator,\n",
    "    tokenizer = feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fd0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 46/150 11:46 < 27:50, 0.06 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.331900</td>\n",
       "      <td>3.324025</td>\n",
       "      <td>0.044444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.315800</td>\n",
       "      <td>3.314242</td>\n",
       "      <td>0.044444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.281200</td>\n",
       "      <td>3.298808</td>\n",
       "      <td>0.044444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history = log_history.fillna(0)\n",
    "log_history = log_history.groupby(['epoch']).sum()\n",
    "log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f362d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history[[\"loss\", \"eval_loss\", \"eval_accuracy\"]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e37ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "test_dataset = load_from_disk(test_save_path)\n",
    "\n",
    "# Load trained model\n",
    "model = ViTForImageClassification.from_pretrained('./model')\n",
    "\n",
    "# Load feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('./model')\n",
    "    \n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=feature_extractor\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# Writes eval_result to file which can be accessed later\n",
    "with open(os.path.join(output_data_dir, \"eval_results.json\"), \"w\") as writer:\n",
    "    print(f\"Logging evaluation results at {output_data_dir}/eval_results.json\")\n",
    "    writer.write(json.dumps(eval_results))\n",
    "\n",
    "print(json.dumps(eval_results, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
