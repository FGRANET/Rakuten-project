{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIzTVUaLN5tS"
   },
   "source": [
    ">## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54459,
     "status": "ok",
     "timestamp": 1692174189649,
     "user": {
      "displayName": "Franck Granet",
      "userId": "04771594420292870409"
     },
     "user_tz": -120
    },
    "id": "duUabS8O4Glc",
    "outputId": "74ff0b98-615e-4835-934f-70f59344f305"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-translate==2.0.1 in c:\\users\\franc\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.1.0 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-cloud-translate==2.0.1) (1.7.3)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.15.0 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-cloud-translate==2.0.1) (1.34.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (1.35.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (1.60.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (2.28.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (3.20.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (1.56.2)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (1.48.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-cloud-core<2.0dev,>=1.1.0->google-cloud-translate==2.0.1) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (63.4.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (4.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\franc\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (0.4.8)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rakuten-390308-9193868c526b.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23652\\1704690503.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mkey_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'rakuten-390308-9193868c526b.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_service_account_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#import stem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\cloud\\client.py\u001b[0m in \u001b[0;36mfrom_service_account_json\u001b[1;34m(cls, json_credentials_path, *args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m                  \u001b[1;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfactory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_credentials_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_fi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             \u001b[0mcredentials_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_fi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rakuten-390308-9193868c526b.json'"
     ]
    }
   ],
   "source": [
    "#Import des packages\n",
    "import pandas as pd\n",
    "\n",
    "#Tokenisation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Sauvegarde\n",
    "from joblib import dump,load\n",
    "\n",
    "#API Google à modifier\n",
    "!pip install google-cloud-translate==2.0.1\n",
    "from google.cloud import translate_v2 as translate\n",
    "\n",
    "key_path = 'rakuten-390308-9193868c526b.json'\n",
    "client = translate.Client.from_service_account_json(key_path)\n",
    "\n",
    "#import stem\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "porter_stemmer = FrenchStemmer()\n",
    "\n",
    "#import lemme\n",
    "#à lancer selon l'environnement\n",
    "\n",
    "!python -m spacy download fr_core_news_md\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "!pip install Unidecode\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfvcUmf8tVuc"
   },
   "source": [
    ">## Fonctions de référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1692174202154,
     "user": {
      "displayName": "Franck Granet",
      "userId": "04771594420292870409"
     },
     "user_tz": -120
    },
    "id": "EqJJHYpv2wSo"
   },
   "outputs": [],
   "source": [
    "#fonction pour gestion de la ponctuation et des balises avant suppression\n",
    "def remplacement(text):\n",
    "  list_ponctuation = [\",\",\".\",\"?\",\"-\",\"!\",\"/\",\"\\\\\",\"'\",\":\",\"!\"]\n",
    "  #ajout d'un espace avant et après ponctuation pour éviter des erreurs de tokenisation\n",
    "  for i in list_ponctuation:\n",
    "    text = text.replace(i,\" \"+i+\" \")\n",
    "  #ajout d'espace avant ou après selon chevron pour éviter concatenation de mots après retrait balise html\n",
    "  text = text.replace(\"<\",\" <\")\n",
    "  text = text.replace(\">\",\"> \")\n",
    "  #gestion des caractères mal encodés hors html (A RETRAVAILLER EN UTILISANT PYENCHANT)\n",
    "  text = text.replace(\"à¢\",\"â\")\n",
    "  text = text.replace(\"â¿¿\",\"'\")\n",
    "  text = text.replace(\"Â°\",\"°\")\n",
    "  text = text.replace(\"¿\",\"oe\")\n",
    "  #suppression encodage html\n",
    "  from bs4 import BeautifulSoup\n",
    "  soup = BeautifulSoup(text, 'html.parser')\n",
    "  text = soup.get_text()\n",
    "  return text\n",
    "\n",
    "def tokenisation(texte):\n",
    "  doc = nlp(texte)\n",
    "  liste = [i.text for i in doc]\n",
    "  return liste\n",
    "\n",
    "#fonction de détection de langue avec API Google\n",
    "def detect_language(text):\n",
    "  result = client.detect_language(text)\n",
    "  return result\n",
    "\n",
    "#Fonction de traduction API Google\n",
    "def translate_list(liste):\n",
    "  resultat = []\n",
    "  for i in liste:\n",
    "    translated_text = client.translate(i, target_language='fr')\n",
    "    resultat.append(translated_text['translatedText'])\n",
    "  return resultat\n",
    "\n",
    "#fonction de tokenisation d'une liste dans une liste\n",
    "def tokenize(liste):\n",
    "  resultat = []\n",
    "  for i in liste:\n",
    "    mot_tokenize =  word_tokenize(i.lower(), language = 'french')\n",
    "    for k in mot_tokenize:\n",
    "      resultat.append(k)\n",
    "  return resultat\n",
    "\n",
    "# fonction de retrait des mots vides (A REPRENDRE AVEC SPACY???)\n",
    "def stop_word_spacy(liste):\n",
    "  stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "  tokens = [i for i in liste if i not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "def nettoyage(tokens):\n",
    "  stop_words = set(stopwords.words('french'))\n",
    "  result=[token for token in tokens if token not in stop_words]\n",
    "  return result\n",
    "\n",
    "#fonction de retrait des mots exclusivement numériques\n",
    "def retrait_chiffres(tokens):\n",
    "  result=[token for token in tokens if not (token.isnumeric ())]\n",
    "  return result\n",
    "\n",
    "#fonction de lemmatisation avec SPACY\n",
    "def lem_word(words):\n",
    "  lemmes = [token.lemma_ for token in nlp(\" \".join(words))]\n",
    "  return lemmes\n",
    "\n",
    "def retrait_accents(liste):\n",
    "  return [unidecode(i) for i in liste]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri1NTyQAemyy"
   },
   "source": [
    "## Fonctions principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VP9wQwQSm2W"
   },
   "outputs": [],
   "source": [
    "def pre_traduction(df):\n",
    "  #création colonne description vide\n",
    "  df['description_vide'] = df['description'].isna()\n",
    "  # fusion de désignation et description\n",
    "  df[\"texte\"] = df.apply(lambda row: row[\"designation\"] + \" \" + row[\"description\"] if not (row[\"description_vide\"] or (row[\"designation\"] == row[\"description\"])) else row[\"designation\"], axis=1)\n",
    "  #retrait des colonnes initiales description et designation\n",
    "  to_drop=['description','designation']\n",
    "  df = df.drop(to_drop,axis=1)\n",
    "  #retrait des doublons dans texte\n",
    "  df = df.drop_duplicates(subset='texte')\n",
    "  #application fonction probleme encodage divers\n",
    "  df[\"texte\"] = df[\"texte\"].apply(remplacement)\n",
    "  #detection de langue\n",
    "  df['langue_texte'] = df['texte'].apply(lambda x: detect_language(x)['language'])\n",
    "  #conservation des langues \"fr\",\"en\",\"es\",\"it\" et \"de\".\n",
    "  df = df.loc[(df['langue_texte']=='fr')|(df['langue_texte']=='en')|(df['langue_texte']=='de')|(df['langue_texte']=='it')|(df['langue_texte']=='es')]\n",
    "  #tokenisation\n",
    "  df['texte'] = df['texte'].astype('str')\n",
    "  df['texte_decoupe']=df['texte'].apply(lambda x: tokenisation(x.lower()))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WqwHSx5r6uB"
   },
   "outputs": [],
   "source": [
    "def traduction(df):\n",
    "  df['texte_decoupe_traduit_fr']=df['texte_decoupe'].apply(translate_list)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJQLiJ3YemhB"
   },
   "outputs": [],
   "source": [
    "def post_traduction(df):\n",
    "  #retrait des mots vides\n",
    "  df['mots'] = df['mots'].apply(stop_word_spacy)\n",
    "  #retrait des accents\n",
    "  df['mots'] = df['mots'].apply(retrait_accents)\n",
    "  #ajout d'une colonne avec retrait des chiffres et sw\n",
    "  df['mots_sans_chiffres'] = df['mots'].apply(retrait_chiffres)\n",
    "  #ajout de deux colonnes de stemming\n",
    "  df[\"mots_stem\"] =  df[\"mots\"].apply(lambda x: [porter_stemmer.stem(i) for i in x])\n",
    "  df[\"mots_sans_chiffres_stem\"] =  df[\"mots_sans_chiffres\"].apply(lambda x: [porter_stemmer.stem(i) for i in x])\n",
    "  #ajout de deux colonnes de lemmatisation\n",
    "  df['mots_lem'] = df['mots'].apply(lem_word)\n",
    "  df[\"mots_sans_chiffres_lem\"] =  df[\"mots_sans_chiffres\"].apply(lem_word)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57-CTDaId7Ol"
   },
   "outputs": [],
   "source": [
    "def regex(df):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IivhESFEykWp"
   },
   "source": [
    ">## Application des fonctions à X_train et X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um7e0pLJtQ0K"
   },
   "source": [
    "Import X_train et X_test et fusion X_train avec y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8KPYqSDyj3u"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"X_train_update.csv\",index_col=0)\n",
    "Y_train = pd.read_csv(\"Y_train_CVw08PX.csv\",index_col=0)\n",
    "#fusion X_train et y_train avant pretraduction\n",
    "df_train = pd.concat([df_train,Y_train], axis=1)\n",
    "X_test_update = pd.read_csv(\"X_test_update.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrAZ9EbYeQOA"
   },
   "outputs": [],
   "source": [
    "df_train_pretraduit = pre_traduction(df_train)\n",
    "df_test_pretraduit = pre_traduction(X_test_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9h6ozS_MhSau"
   },
   "outputs": [],
   "source": [
    "df_test_pretraduit=load('df_test_pretraduit.joblib', mmap_mode=None)\n",
    "df_train_pretraduit=load('df_train_pretraduit.joblib', mmap_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzeXELDBtNHU"
   },
   "source": [
    "Traduction Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bfm9rRK5ongV"
   },
   "outputs": [],
   "source": [
    "#séparation du df selon langue française\n",
    "df_train_non_fr = df_train_pretraduit.loc[df_train_pretraduit['langue_texte'] != 'fr']\n",
    "df_train_fr = df_train_pretraduit.loc[df_train_pretraduit['langue_texte'] == 'fr']\n",
    "#découpage du df non français pour procéder à une traduction par étapes\n",
    "df1_train_non_fr = df_train_non_fr.iloc[0:2000,:]\n",
    "df2_train_non_fr  = df_train_non_fr.iloc[2000:4000,:]\n",
    "df3_train_non_fr  = df_train_non_fr.iloc[4000:6000,:]\n",
    "df4_train_non_fr  = df_train_non_fr.iloc[6000:8000,:]\n",
    "df5_train_non_fr  = df_train_non_fr.iloc[8000:10000,:]\n",
    "df6_train_non_fr  = df_train_non_fr.iloc[10000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJvG9o4ppZ6R"
   },
   "outputs": [],
   "source": [
    "df1_train_traduit = traduction(df1_train_non_fr)\n",
    "dump(df1_train_traduit,'df1_train_traduit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhFKVyfLpowo"
   },
   "outputs": [],
   "source": [
    "df2_train_traduit = traduction(df2_train_non_fr)\n",
    "dump(df2_train_traduit,'df2_train_traduit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GexwhA54poui"
   },
   "outputs": [],
   "source": [
    "df3_train_traduit = traduction(df3_train_non_fr)\n",
    "dump(df3_train_traduit,'df3_train_traduit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptw7by9LposZ"
   },
   "outputs": [],
   "source": [
    "df4_train_traduit = traduction(df4_train_non_fr)\n",
    "dump(df4_train_traduit,'df4_train_traduit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgFBsxrEpoqN"
   },
   "outputs": [],
   "source": [
    "df5_train_traduit = traduction(df5_train_non_fr)\n",
    "dump(df5_train_traduit,'df5_train_traduit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVhoKnRipooK"
   },
   "outputs": [],
   "source": [
    "df6_train_traduit = traduction(df6_train_non_fr)\n",
    "dump(df6_train_traduit,'df6_train_traduit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kw3KidcxjWdj"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23652\\2087054409.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1_train_traduit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"df1_train_traduit.joblib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf2_train_traduit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df2_train_traduit.joblib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf3_train_traduit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df3_train_traduit.joblib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf4_train_traduit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df4_train_traduit.joblib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf5_train_traduit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df5_train_traduit.joblib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1210\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m                 \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1212\u001b[1;33m                 \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1213\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "df1_train_traduit=load(\"df1_train_traduit.joblib\", mmap_mode=None)\n",
    "df2_train_traduit=load('df2_train_traduit.joblib', mmap_mode=None)\n",
    "df3_train_traduit=load('df3_train_traduit.joblib', mmap_mode=None)\n",
    "df4_train_traduit=load('df4_train_traduit.joblib', mmap_mode=None)\n",
    "df5_train_traduit=load('df5_train_traduit.joblib', mmap_mode=None)\n",
    "df6_train_traduit=load('df6_train_traduit.joblib', mmap_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eny3cp98pojP"
   },
   "outputs": [],
   "source": [
    "#concaténation des parties\n",
    "df_texte_non_fr_traduit = pd.concat([df1_train_traduit,df2_train_traduit,\n",
    "                                     df3_train_traduit,df4_train_traduit,\n",
    "                                     df5_train_traduit,df6_train_traduit],axis=0)\n",
    "#tokenisation aprés traduction car ajout de liste dans la liste\n",
    "df_texte_non_fr_traduit[\"mots\"] = df_texte_non_fr_traduit[\"mots\"].apply(lambda x:tokenize(x))\n",
    "#fusion des textes non traduits et traduits\n",
    "dict_1 = {'texte_decoupe_traduit_fr':'mots'}\n",
    "df_texte_non_fr_traduit.rename(dict_1,axis=1,inplace=True)\n",
    "dict_2 = {'texte_decoupe':'mots'}\n",
    "df_fr.rename(dict_2,axis=1,inplace=True)\n",
    "#fusion des deux df\n",
    "df_train = pd.concat([df_fr, df_texte_non_fr_traduit],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKb0phKJuOQM"
   },
   "outputs": [],
   "source": [
    "dump(df_train,'df_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yuug6eKPtG0Y"
   },
   "source": [
    "Traduction TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Myvj-oUrrFk"
   },
   "outputs": [],
   "source": [
    "#séparation du df_test selon langue française\n",
    "df_test_non_fr = df_test_pretraduit.loc[df_test_pretraduit['langue_texte'] != 'fr']\n",
    "df_test_fr = df_test_pretraduit.loc[df_test_pretraduit['langue_texte'] == 'fr']\n",
    "#découpage du df non français pour procéder à une traduction par étapes\n",
    "df1_test_non_fr = df_test_non_fr.iloc[0:800,:]\n",
    "df2_test_non_fr = df_test_non_fr.iloc[800:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPyPEpYhtB9V"
   },
   "outputs": [],
   "source": [
    "df1_test_traduit = traduction(df1_test_non_fr)\n",
    "dump(df1_test_traduit,'df1_test_traduit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwH3WCa6tBd8"
   },
   "outputs": [],
   "source": [
    "df2_test_traduit = traduction(df2_test_non_fr)\n",
    "dump(df2_test_traduit,'df2_test_traduit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JiUuz1JlEDs"
   },
   "outputs": [],
   "source": [
    "df1_test_traduit=load('df1_test_traduit.joblib', mmap_mode=None)\n",
    "df2_test_traduit=load('df2_test_traduit.joblib', mmap_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0pOVZdjtq4D"
   },
   "outputs": [],
   "source": [
    "#concaténation des parties à modifier ....\n",
    "df_test_texte_non_fr_traduit = pd.concat([df1_test_traduit,df2_test_traduit],axis=0)\n",
    "#tokenisation aprés traduction car ajout de liste dans la liste\n",
    "df_test_texte_non_fr_traduit[\"mots\"] = df_test_texte_non_fr_traduit[\"mots\"].apply(lambda x:tokenize(x))\n",
    "#fusion des textes non traduits et traduits\n",
    "dict_1 = {'texte_decoupe_traduit_fr':'mots'}\n",
    "df_test_texte_non_fr_traduit.rename(dict_1,axis=1,inplace=True)\n",
    "dict_2 = {'texte_decoupe':'mots'}\n",
    "df_fr.rename(dict_2,axis=1,inplace=True)\n",
    "#fusion des deux df\n",
    "df_test = pd.concat([df_fr, df_test_texte_non_fr_traduit],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62lt5AX7uVm6"
   },
   "outputs": [],
   "source": [
    "dump(df_test,'df_test.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGlPh5dMtgkZ"
   },
   "source": [
    "Traitement post traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfQMBieCeTt1"
   },
   "outputs": [],
   "source": [
    "df_train = post_traduction(df_train)\n",
    "df_test = post_traduction(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKYsiXB2y04p"
   },
   "source": [
    ">## Sauvegarde avant choix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ql-0hiyIzFqq"
   },
   "outputs": [],
   "source": [
    "dump(df, 'df_github.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHkrPems8Ktr"
   },
   "outputs": [],
   "source": [
    "###Language detection with Spacy\n",
    "'''\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "from spacy_language_detection import LanguageDetector\n",
    "\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector(seed=42)\n",
    "\n",
    "\n",
    "nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp_model.add_pipe('language_detector', last=True)\n",
    "\n",
    "for r in zip(df['texte']):\n",
    " df1=pd.concat([df1,pd.DataFrame([nlp_model(r[0])._.language])],ignore_index=True)\n",
    "\n",
    "''''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1RRw18oEsNkWhojZffra-P1JqYC6kOvo2",
     "timestamp": 1691584001183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
