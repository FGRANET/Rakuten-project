{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92945113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation du texte sous forme de liste en string\n",
    "def jonction(liste):\n",
    "  result = \"\"\n",
    "  for token in liste:\n",
    "    result = result + token + \" \"\n",
    "  return result\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import load,dump\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import time\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def optimisation_model_avec_regex(df_initial,colonne_a_vectoriser,liste_colonne_regex,max_features,k_best,joblib_path_suivi_metrique,grid_search_cv):\n",
    "    #import du df\n",
    "    df = load(df_initial)\n",
    "    #transformation de la future colonne feature en chaine de caractère\n",
    "    df[colonne_a_vectoriser]=df[colonne_a_vectoriser].apply(jonction)\n",
    "    #séparation des features et de la cible\n",
    "    liste_colonne_a_vectoriser = [colonne_a_vectoriser]\n",
    "    data = df[liste_colonne_a_vectoriser+liste_colonne_regex]\n",
    "    target = df[\"prdtypecode\"]\n",
    "    #séparation de train et test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42, stratify = target)\n",
    "    #vectorisation selon un nombre max de mots les plus fréquents\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    X_train_regex = X_train[liste_colonne_regex]\n",
    "    X_train_vec = pd.DataFrame(vectorizer.fit_transform(X_train[colonne_a_vectoriser]).todense(),index=X_train_regex.index)\n",
    "    X_train = pd.concat([X_train_regex,X_train_vec],axis = 1)\n",
    "    X_test_regex = X_test[liste_colonne_regex]\n",
    "    X_test_vec = pd.DataFrame(vectorizer.transform(X_test[colonne_a_vectoriser]).todense(),index=X_test_regex.index)\n",
    "    X_test = pd.concat([X_test_regex,X_test_vec],axis = 1)\n",
    "    X_train.columns = X_train.columns.astype(str)\n",
    "    X_test.columns = X_test.columns.astype(str)\n",
    "    scaler = StandardScaler().fit(np.asarray(X_train))\n",
    "    X_train = scaler.transform(np.asarray(X_train))\n",
    "    X_test = scaler.transform(np.asarray(X_test))\n",
    "    sel = SelectKBest(k=k_best)\n",
    "    sel.fit(np.asarray(X_train),y_train)\n",
    "    mask = sel.get_support()\n",
    "    X_train = sel.transform(np.asarray(X_train))\n",
    "    X_test = sel.transform(np.asarray(X_test))\n",
    "    if os.path.exists(joblib_path_suivi_metrique):\n",
    "        df_import = load(joblib_path_suivi_metrique)\n",
    "        print(\"récupération du df existant\")\n",
    "    else:\n",
    "        df_import = pd.DataFrame(columns=[\"Max_features\",\"K_best\",\"Grid\",\"Hyperparamètres\",\"Best_param\", \"Accuracy\", \"F1_weighted\", \"F1_macro\", \"Duree en sec\"])\n",
    "        print(\"création d'un dataframe\")\n",
    "    score = []\n",
    "    print(\"debut du grid:\",grid_search_cv)\n",
    "    debut = time.time()\n",
    "    grid_search_cv.fit(np.asarray(X_train),y_train)\n",
    "    print(\"fin du grid:\",grid_search_cv)\n",
    "    print(\"debut meilleur param:\",grid_search_cv.best_params_)\n",
    "    best_params = grid_search_cv.best_params_\n",
    "    best_rf_model = RandomForestClassifier(**best_params)\n",
    "    best_rf_model.fit(np.asarray(X_train), y_train)\n",
    "    y_pred  = best_rf_model.predict(np.asarray(X_test))\n",
    "    print(\"fin meilleur param\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    fin = time.time()\n",
    "    duree = fin - debut\n",
    "    model_scores = { \n",
    "            \"Max_features\": f\"{max_features}\",\n",
    "            \"K_best\":f\"{k_best}\",\n",
    "            \"Grid\": f\"{grid_search_cv}\",\n",
    "            \"Hyperparamètres\":f\"{param_grid}\",\n",
    "            \"Best_param\":f\"{best_params}\",\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"F1_weighted\": f1_weighted,\n",
    "            \"F1_macro\": f1_macro,\n",
    "            \"Duree en sec\": duree}\n",
    "    score.append(model_scores)\n",
    "    df_score = pd.DataFrame(score)\n",
    "    df = pd.concat([df_import, df_score], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e13ca",
   "metadata": {},
   "source": [
    "Verification de l'importance des colonnes regex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1531cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sur les 13 colonnes de regex on en retient : {12}\n"
     ]
    }
   ],
   "source": [
    "df_initial = r\"C:\\Users\\franc\\AutoML\\df_post_trad_et_regex.joblib\"\n",
    "colonne_a_vectoriser = \"mots_stem_sans_chiffres\"\n",
    "liste_colonne_regex = ['masse_encod_freq', 'surface_encod_freq', 'vitesse_encod_freq',\n",
    "       'pression_encod_freq', 'energie_alim_encod_freq',\n",
    "       'energie_elec_encod_freq', 'longueur_encod_freq', 'volume_encod_freq',\n",
    "       'memoire_encod_freq', 'temps_encod_freq', 'chiffres_encod_freq',\n",
    "       'description_vide_encod_freq', 'Nb_mots_cat_encod_freq']\n",
    "max_features = 4000\n",
    "k_best = 3000\n",
    "df = load(df_initial)\n",
    "#transformation de la future colonne feature en chaine de caractère\n",
    "df[colonne_a_vectoriser]=df[colonne_a_vectoriser].apply(jonction)\n",
    "#séparation des features et de la cible\n",
    "liste_colonne_a_vectoriser = [colonne_a_vectoriser]\n",
    "data = df[liste_colonne_a_vectoriser+liste_colonne_regex]\n",
    "target = df[\"prdtypecode\"]\n",
    "#séparation de train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42, stratify = target)\n",
    "#vectorisation selon un nombre max de mots les plus fréquents\n",
    "vectorizer = CountVectorizer(max_features=max_features)\n",
    "X_train_regex = X_train[liste_colonne_regex]\n",
    "X_train_vec = pd.DataFrame(vectorizer.fit_transform(X_train[colonne_a_vectoriser]).todense(),index=X_train_regex.index)\n",
    "X_train = pd.concat([X_train_regex,X_train_vec],axis = 1)\n",
    "X_test_regex = X_test[liste_colonne_regex]\n",
    "X_test_vec = pd.DataFrame(vectorizer.transform(X_test[colonne_a_vectoriser]).todense(),index=X_test_regex.index)\n",
    "X_test = pd.concat([X_test_regex,X_test_vec],axis = 1)\n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "sel = SelectKBest(k=k_best)\n",
    "sel.fit(X_train,y_train)\n",
    "mask = sel.get_support()\n",
    "X_train_sel = sel.transform(X_train)\n",
    "X_test_sel = sel.transform(X_test)  \n",
    "    \n",
    "liste_columns_retenues = []\n",
    "for i,j in enumerate(mask):\n",
    "    if j:\n",
    "        liste_columns_retenues.append(X_train.columns[i])  \n",
    "\n",
    "\n",
    "s=0\n",
    "for i in liste_colonne_regex:\n",
    "    if i in liste_columns_retenues:\n",
    "        s+=1\n",
    "print(f' Sur les 13 colonnes de regex on en retient :' ,{s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7505563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rdf = RandomForestClassifier(n_jobs = -1,random_state = 23)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50],  # Nombre d'arbres dans la forêt\n",
    "    'criterion': [\"log_loss\"],  # Métrique de qualité de la division\n",
    "    'max_depth': [None],  # Profondeur maximale de chaque arbre\n",
    "    'min_samples_split': [5],  # Nombre minimal d'échantillons requis pour diviser un nœud\n",
    "    'min_samples_leaf': [1],  # Nombre minimal d'échantillons requis pour être une feuille\n",
    "    'max_features': ['log2'],  # Nombre maximal de fonctionnalités à considérer pour la division\n",
    "    'bootstrap': [False],  # Si l'échantillonnage bootstrap doit être utilisé\n",
    "    'class_weight': ['balanced'],  # Poids des classes pour la correction de déséquilibre\n",
    "    'random_state': [23],  # Seed pour la reproductibilité\n",
    "    'n_jobs': [-1]  # Utilisation de tous les cœurs de CPU}\n",
    "}\n",
    "\n",
    "\n",
    "gcv = GridSearchCV(estimator=rdf, param_grid = param_grid,cv=5, n_jobs=-1, scoring=\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6d7016b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "création d'un dataframe\n",
      "debut du grid: GridSearchCV(cv=5, estimator=RandomForestClassifier(n_jobs=-1, random_state=23),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'bootstrap': [False], 'class_weight': ['balanced'],\n",
      "                         'criterion': ['log_loss'], 'max_depth': [None],\n",
      "                         'max_features': ['log2'], 'min_samples_leaf': [1],\n",
      "                         'min_samples_split': [5], 'n_estimators': [50],\n",
      "                         'n_jobs': [-1], 'random_state': [23]},\n",
      "             scoring='f1_weighted')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 348, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"C:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 622, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"C:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 915, in check_array\n",
      "    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n",
      "  File \"C:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 380, in _asarray_with_order\n",
      "    array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 596. MiB for an array with shape (52112, 3000) and data type float32\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\franc\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin du grid: GridSearchCV(cv=5, estimator=RandomForestClassifier(n_jobs=-1, random_state=23),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'bootstrap': [False], 'class_weight': ['balanced'],\n",
      "                         'criterion': ['log_loss'], 'max_depth': [None],\n",
      "                         'max_features': ['log2'], 'min_samples_leaf': [1],\n",
      "                         'min_samples_split': [5], 'n_estimators': [50],\n",
      "                         'n_jobs': [-1], 'random_state': [23]},\n",
      "             scoring='f1_weighted')\n",
      "debut meilleur param: {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'log_loss', 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50, 'n_jobs': -1, 'random_state': 23}\n",
      "fin meilleur param\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\franc\\\\AutoML\\\\df_optimisation_avec_regex.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_initial = r\"C:\\Users\\franc\\AutoML\\df_post_trad_et_regex.joblib\"\n",
    "colonne_a_vectoriser = \"mots_stem_sans_chiffres\"\n",
    "liste_colonne_regex = ['masse_encod_freq', 'surface_encod_freq', 'vitesse_encod_freq',\n",
    "       'pression_encod_freq', 'energie_alim_encod_freq',\n",
    "       'energie_elec_encod_freq', 'longueur_encod_freq', 'volume_encod_freq',\n",
    "       'memoire_encod_freq', 'temps_encod_freq', 'chiffres_encod_freq',\n",
    "       'description_vide_encod_freq', 'Nb_mots_cat_encod_freq']\n",
    "max_features = 4000\n",
    "k_best = 3000\n",
    "joblib_path_suivi_metrique = r\"C:\\Users\\franc\\AutoML\\df_optimisation_avec_regex.joblib\"\n",
    "grid_search_cv = gcv\n",
    "df_optimisation_avec_regex = optimisation_model_avec_regex(df_initial,colonne_a_vectoriser,liste_colonne_regex,max_features,k_best,joblib_path_suivi_metrique,grid_search_cv)\n",
    "dump(df_optimisation_avec_regex,r\"C:\\Users\\franc\\AutoML\\df_optimisation_avec_regex.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f753f8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max_features</th>\n",
       "      <th>K_best</th>\n",
       "      <th>Grid</th>\n",
       "      <th>Hyperparamètres</th>\n",
       "      <th>Best_param</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_weighted</th>\n",
       "      <th>F1_macro</th>\n",
       "      <th>Duree en sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4000</td>\n",
       "      <td>3000</td>\n",
       "      <td>GridSearchCV(cv=5, estimator=RandomForestClass...</td>\n",
       "      <td>{'n_estimators': [50], 'criterion': ['log_loss...</td>\n",
       "      <td>{'bootstrap': False, 'class_weight': 'balanced...</td>\n",
       "      <td>0.775329</td>\n",
       "      <td>0.773499</td>\n",
       "      <td>0.755715</td>\n",
       "      <td>174.347588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Max_features K_best                                               Grid  \\\n",
       "0         4000   3000  GridSearchCV(cv=5, estimator=RandomForestClass...   \n",
       "\n",
       "                                     Hyperparamètres  \\\n",
       "0  {'n_estimators': [50], 'criterion': ['log_loss...   \n",
       "\n",
       "                                          Best_param  Accuracy  F1_weighted  \\\n",
       "0  {'bootstrap': False, 'class_weight': 'balanced...  0.775329     0.773499   \n",
       "\n",
       "   F1_macro  Duree en sec  \n",
       "0  0.755715    174.347588  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_optimisation_avec_regex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
