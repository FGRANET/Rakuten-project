{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIzTVUaLN5tS"
   },
   "source": [
    ">## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duUabS8O4Glc"
   },
   "outputs": [],
   "source": [
    "#Import des packages\n",
    "import pandas as pd\n",
    "\n",
    "#Tokenisation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Sauvegarde\n",
    "from joblib import dump,load\n",
    "\n",
    "#API Google\n",
    "!pip install google-cloud-translate==2.0.1\n",
    "from google.cloud import translate_v2 as translate\n",
    "\n",
    "key_path = 'rakuten-390308-9193868c526b.json'\n",
    "client = translate.Client.from_service_account_json(key_path)\n",
    "\n",
    "#import stem\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "porter_stemmer = FrenchStemmer()\n",
    "\n",
    "#import lemme\n",
    "#à lancer selon l'environnement\n",
    "\n",
    "!python -m spacy download fr_core_news_md\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "!pip install Unidecode\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfvcUmf8tVuc"
   },
   "source": [
    ">## Fonctions de référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqJJHYpv2wSo"
   },
   "outputs": [],
   "source": [
    "#fonction pour gestion de la ponctuation et des balises avant suppression\n",
    "def remplacement(text):\n",
    "  list_ponctuation = [\",\",\".\",\"?\",\"-\",\"!\",\"/\",\"\\\\\",\"'\",\":\",\"!\"]\n",
    "  #ajout d'un espace avant et après ponctuation pour éviter des erreurs de tokenisation\n",
    "  for i in list_ponctuation:\n",
    "    text = text.replace(i,\" \"+i+\" \")\n",
    "  #ajout d'espace avant ou après selon chevron pour éviter concatenation de mots après retrait balise html\n",
    "  text = text.replace(\"<\",\" <\")\n",
    "  text = text.replace(\">\",\"> \")\n",
    "  #gestion des caractères mal encodés hors html (A RETRAVAILLER EN UTILISANT PYENCHANT)\n",
    "  text = text.replace(\"à¢\",\"â\")\n",
    "  text = text.replace(\"â¿¿\",\"'\")\n",
    "  text = text.replace(\"Â°\",\"°\")\n",
    "  text = text.replace(\"¿\",\"oe\")\n",
    "  #suppression encodage html\n",
    "  from bs4 import BeautifulSoup\n",
    "  soup = BeautifulSoup(text, 'html.parser')\n",
    "  text = soup.get_text()\n",
    "  return text\n",
    "\n",
    "def encodage_html_liste(liste):\n",
    "  from bs4 import BeautifulSoup\n",
    "  nouvelle_liste=[]\n",
    "  for i in liste:\n",
    "    nouvelle_sous_liste=[]\n",
    "    for j in i:\n",
    "      soup = BeautifulSoup(j, 'html.parser')\n",
    "      nouvelle_sous_liste.append(soup.get_text() )\n",
    "    nouvelle_liste.append(nouvelle_sous_liste)\n",
    "  return nouvelle_liste\n",
    "\n",
    "\n",
    "def tokenisation(texte):\n",
    "  doc = nlp(texte)\n",
    "  liste = [i.text for i in doc]\n",
    "  return liste\n",
    "\n",
    "#fonction de détection de langue avec API Google\n",
    "def detect_language(text):\n",
    "  result = client.detect_language(text)\n",
    "  return result\n",
    "\n",
    "#Fonction de traduction API Google\n",
    "def translate_list(liste):\n",
    "  resultat = []\n",
    "  for i in liste:\n",
    "    translated_text = client.translate(i, target_language='fr')\n",
    "    resultat.append(translated_text['translatedText'])\n",
    "  return resultat\n",
    "\n",
    "#fonction de tokenisation d'une liste dans une liste\n",
    "def tokenize(liste):\n",
    "  resultat = []\n",
    "  for i in liste:\n",
    "    mot_tokenize =  word_tokenize(i.lower(), language = 'french')\n",
    "    for k in mot_tokenize:\n",
    "      resultat.append(k)\n",
    "  return resultat\n",
    "\n",
    "# fonction de retrait des mots vides (A REPRENDRE AVEC SPACY???)\n",
    "def stop_word_spacy(liste):\n",
    "  stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "  tokens = [i for i in liste if i not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "def nettoyage(tokens):\n",
    "  stop_words = set(stopwords.words('french'))\n",
    "  result=[token for token in tokens if token not in stop_words]\n",
    "  return result\n",
    "\n",
    "#fonction de retrait des caractères non alpha-numeriques\n",
    "\n",
    "def alphanum(tokens):\n",
    "  result = [token for token in tokens if token.isalnum()]\n",
    "  return result\n",
    "\n",
    "#fonction de retrait des mots exclusivement numériques\n",
    "def retrait_chiffres(tokens):\n",
    "  result=[token for token in tokens if not (token.isnumeric ())]\n",
    "  return result\n",
    "\n",
    "def retrait_accents(liste):\n",
    "  return [unidecode(i) for i in liste]\n",
    "\n",
    "def retrait_mail(s):\n",
    "  s = re.sub(r'http\\S*','',s)\n",
    "  return(s)\n",
    "\n",
    "#fonction de lemmatisation avec SPACY\n",
    "def lem_word(words):\n",
    "  lemmes = [token.lemma_ for token in nlp(\" \".join(words))]\n",
    "  return lemmes\n",
    "\n",
    "#fonction de stemming\n",
    "\n",
    "def stemming(liste):\n",
    "  result = [porter_stemmer.stem(i) for i in liste]\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri1NTyQAemyy"
   },
   "source": [
    "## Fonctions principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VP9wQwQSm2W"
   },
   "outputs": [],
   "source": [
    "def pre_traduction(df):\n",
    "  #création colonne description vide\n",
    "  df['description_vide'] = df['description'].isna()\n",
    "  # fusion de désignation et description\n",
    "  df[\"texte\"] = df.apply(lambda row: row[\"designation\"] + \" \" + row[\"description\"] if not (row[\"description_vide\"] or (row[\"designation\"] == row[\"description\"])) else row[\"designation\"], axis=1)\n",
    "  #retrait des colonnes initiales description et designation\n",
    "  to_drop=['description','designation']\n",
    "  df = df.drop(to_drop,axis=1)\n",
    "  #retrait des doublons dans texte\n",
    "  df = df.drop_duplicates(subset='texte')\n",
    "  #application fonction probleme encodage divers\n",
    "  df[\"texte\"] = df[\"texte\"].apply(remplacement)\n",
    "  #detection de langue\n",
    "  df['langue_texte'] = df['texte'].apply(lambda x: detect_language(x)['language'])\n",
    "  #conservation des langues \"fr\",\"en\",\"es\",\"it\" et \"de\".\n",
    "  df = df.loc[(df['langue_texte']=='fr')|(df['langue_texte']=='en')|(df['langue_texte']=='de')|(df['langue_texte']=='it')|(df['langue_texte']=='es')]\n",
    "  #tokenisation\n",
    "  df['texte'] = df['texte'].astype('str')\n",
    "  df['texte_decoupe']=df['texte'].apply(lambda x: tokenisation(x.lower()))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WqwHSx5r6uB"
   },
   "outputs": [],
   "source": [
    "def traduction(df):\n",
    "  df['texte_decoupe_traduit_fr']=df['texte_decoupe'].apply(translate_list)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJQLiJ3YemhB"
   },
   "outputs": [],
   "source": [
    "def post_traduction(df):\n",
    "  #retrait des colonnes texte et texte découpé\n",
    "  df = df.drop(columns=['texte','texte_decoupe'])\n",
    "  #retrait des balises à une liste de liste de mots\n",
    "  df[\"mots\"] = df[\"mots\"].apply(encodage_html_liste)\n",
    "  #retokenisation\n",
    "  df[\"mots\"] = df[\"mots\"].apply(tokenize)\n",
    "  #retrait des mots vides\n",
    "  df['mots'] = df['mots'].apply(stop_word_spacy)\n",
    "  #retrait des accents\n",
    "  df['mots'] = df['mots'].apply(retrait_accents)\n",
    "  #suppression des caractères non alpha numériques\n",
    "  df[\"mots\"] = df[\"mots\"].apply(alphanum)\n",
    "  #détection et retrait adresse mail\n",
    "  df['mots'] = df['mots'].apply(retrait_mail)\n",
    "  #ajout de deux colonnes de stemming\n",
    "  df[\"mots_stem\"] =  df[\"mots\"].apply(stemming)\n",
    "  df[\"mots_stem_sans_chiffres\"] =  df[\"mots_stem\"].apply(retrait_chiffres)\n",
    "  #ajout de deux colonnes de lemmatisation\n",
    "  df['mots_lem'] = df['mots'].apply(lem_word)\n",
    "  df[\"mots_lem_sans_chiffres\"] =  df[\"mots_lem\"].apply(retrait_chiffres)\n",
    "  #ajout d'une colonne qui stem apres lem\n",
    "  df[\"mots_lem_stem\"] = df[\"mots_lem\"].apply(stemming)\n",
    "  df[\"mots_lem_stem_sans_chiffres\"] = df[\"mots_lem_stem\"].apply(retrait_chiffres)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IivhESFEykWp"
   },
   "source": [
    ">## Application des fonctions à X_train et X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um7e0pLJtQ0K"
   },
   "source": [
    "Import X_train et X_test et fusion X_train avec y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8KPYqSDyj3u"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"X_train_update.csv\",index_col=0)\n",
    "Y_train = pd.read_csv(\"Y_train_CVw08PX.csv\",index_col=0)\n",
    "#fusion X_train et y_train avant pretraduction\n",
    "df_train = pd.concat([df_train,Y_train], axis=1)\n",
    "X_test_update = pd.read_csv(\"X_test_update.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1OBDi-66Mbo"
   },
   "source": [
    "Appliquer la Fonction pretraduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrAZ9EbYeQOA"
   },
   "outputs": [],
   "source": [
    "df_train_pretraduit = pre_traduction(df_train)\n",
    "dump(df_train_pretraduit,'df_train_pretraduit.joblib')\n",
    "\n",
    "df_test_pretraduit = pre_traduction(X_test_update)\n",
    "dump(df_test_pretraduit,'df_test_pretraduit.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9h6ozS_MhSau"
   },
   "outputs": [],
   "source": [
    "df_test_pretraduit=load(\"/content/drive/MyDrive/Rakuten/_df_Traduit_Final/df_test_pretraduit.joblib\")\n",
    "df_train_pretraduit=load(\"/content/drive/MyDrive/Rakuten/_df_Traduit_Final/df_train_pretraduit.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzeXELDBtNHU"
   },
   "source": [
    "Traduction Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bfm9rRK5ongV"
   },
   "outputs": [],
   "source": [
    "#séparation du df selon langue française\n",
    "df_train_non_fr = df_train_pretraduit.loc[df_train_pretraduit['langue_texte'] != 'fr']\n",
    "df_train_fr = df_train_pretraduit.loc[df_train_pretraduit['langue_texte'] == 'fr']\n",
    "\n",
    "#découpage du df non français pour procéder à une traduction par étapes\n",
    "df1_train_non_fr = df_train_non_fr.iloc[0:2000,:]\n",
    "df2_train_non_fr  = df_train_non_fr.iloc[2000:4000,:]\n",
    "df3_train_non_fr  = df_train_non_fr.iloc[4000:6000,:]\n",
    "df4_train_non_fr  = df_train_non_fr.iloc[6000:8000,:]\n",
    "df5_train_non_fr  = df_train_non_fr.iloc[8000:10000,:]\n",
    "df6_train_non_fr  = df_train_non_fr.iloc[10000:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJvG9o4ppZ6R"
   },
   "outputs": [],
   "source": [
    "#Appliquer la fonction traduction\n",
    "df1_train_traduit = traduction(df1_train_non_fr)\n",
    "dump(df1_train_traduit,'df1_train_traduit.joblib')\n",
    "df2_train_traduit = traduction(df2_train_non_fr)\n",
    "dump(df2_train_traduit,'df2_train_traduit.joblib')\n",
    "df3_train_traduit = traduction(df3_train_non_fr)\n",
    "dump(df3_train_traduit,'df3_train_traduit.joblib')\n",
    "df4_train_traduit = traduction(df4_train_non_fr)\n",
    "dump(df4_train_traduit,'df4_train_traduit.joblib')\n",
    "df5_train_traduit = traduction(df5_train_non_fr)\n",
    "dump(df5_train_traduit,'df5_train_traduit.joblib')\n",
    "df6_train_traduit = traduction(df6_train_non_fr)\n",
    "dump(df6_train_traduit,'df6_train_traduit.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhFKVyfLpowo"
   },
   "outputs": [],
   "source": [
    "df1_train_traduit=load(\"/content/drive/MyDrive/Rakuten/_df_Traduit_Final/df1_train_traduit.joblib\")\n",
    "df2_train_traduit=load(\"/content/drive/MyDrive/Rakuten/_df_Traduit_Final/df2_train_traduit.joblib\")\n",
    "df3_train_traduit=load(\"/content/drive/MyDrive/Rakuten/_df_Traduit_Final/df3_train_traduit.joblib\")\n",
    "df4_train_traduit=load(\"/content/drive/MyDrive/Rakuten/_df_Traduit_Final/df4_train_traduit.joblib\")\n",
    "df5_train_traduit=load(\"/content/drive/MyDrive/Rakuten/_df_Traduit_Final/df5_train_traduit.joblib\")\n",
    "df6_train_traduit=load(\"/content/drive/MyDrive/Rakuten/_df_Traduit_Final/df6_train_traduit.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgFBsxrEpoqN"
   },
   "outputs": [],
   "source": [
    "#concaténation des parties\n",
    "df_texte_non_fr_traduit = pd.concat([df1_train_traduit,df2_train_traduit,\n",
    "                                     df3_train_traduit,df4_train_traduit,\n",
    "                                     df5_train_traduit,df6_train_traduit],axis=0)\n",
    "#fusion des textes non traduits et traduits\n",
    "dict_1 = {'texte_decoupe_traduit_fr':'mots'}\n",
    "df_texte_non_fr_traduit.rename(dict_1,axis=1,inplace=True)\n",
    "dict_2 = {'texte_decoupe':'mots'}\n",
    "df_train_fr.rename(dict_2,axis=1,inplace=True)\n",
    "#fusion des deux df\n",
    "df_train = pd.concat([df_train_fr, df_texte_non_fr_traduit],axis=0)\n",
    "dump(df_train,'df_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yuug6eKPtG0Y"
   },
   "source": [
    "Traduction TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Myvj-oUrrFk"
   },
   "outputs": [],
   "source": [
    "#séparation du df_test selon langue française\n",
    "df_test_non_fr = df_test_pretraduit.loc[df_test_pretraduit['langue_texte'] != 'fr']\n",
    "df_test_fr = df_test_pretraduit.loc[df_test_pretraduit['langue_texte'] == 'fr']\n",
    "\n",
    "#découpage du df non français pour procéder à une traduction par étapes\n",
    "df1_test_non_fr = df_test_non_fr.iloc[0:800,:]\n",
    "df2_test_non_fr = df_test_non_fr.iloc[800:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPyPEpYhtB9V"
   },
   "outputs": [],
   "source": [
    "df1_test_traduit = traduction(df1_test_non_fr)\n",
    "dump(df1_test_traduit,'df1_test_traduit.joblib')\n",
    "df2_test_traduit = traduction(df2_test_non_fr)\n",
    "dump(df2_test_traduit,'df2_test_traduit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JiUuz1JlEDs"
   },
   "outputs": [],
   "source": [
    "df1_test_traduit=load(\"/content/drive/MyDrive/Rakuten/_df_Traduit_Final/df1_test_traduit.joblib\")\n",
    "df2_test_traduit=load(\"/content/drive/MyDrive/Rakuten/_df_Traduit_Final/df2_test_traduit.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0pOVZdjtq4D"
   },
   "outputs": [],
   "source": [
    "#concaténation des parties\n",
    "df_test_texte_non_fr_traduit = pd.concat([df1_test_traduit,df2_test_traduit],axis=0)\n",
    "#fusion des textes non traduits et traduits\n",
    "dict_1 = {'texte_decoupe_traduit_fr':'mots'}\n",
    "df_test_texte_non_fr_traduit.rename(dict_1,axis=1,inplace=True)\n",
    "dict_2 = {'texte_decoupe':'mots'}\n",
    "df_test_fr.rename(dict_2,axis=1,inplace=True)\n",
    "#fusion des deux df\n",
    "df_test = pd.concat([df_test_texte_non_fr_traduit,df_test_fr],axis=0)\n",
    "dump(df_test,'df_test.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGlPh5dMtgkZ"
   },
   "source": [
    "Traitement post traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LDsQOJy95sh"
   },
   "outputs": [],
   "source": [
    "df_train = load('df_train.joblib')\n",
    "df_test = load('df_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5_JhnUHLGcK"
   },
   "outputs": [],
   "source": [
    "tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfQMBieCeTt1"
   },
   "outputs": [],
   "source": [
    "df_train = post_traduction(df_train)\n",
    "df_test = post_traduction(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKYsiXB2y04p"
   },
   "source": [
    ">## Sauvegarde avant regex et ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ql-0hiyIzFqq"
   },
   "outputs": [],
   "source": [
    "dump(df_train,'df_train_post_trad.joblib')\n",
    "dump(df_test,'df_test_post_trad.joblib')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1RRw18oEsNkWhojZffra-P1JqYC6kOvo2",
     "timestamp": 1691584001183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
