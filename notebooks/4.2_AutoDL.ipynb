{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51421ee",
   "metadata": {},
   "source": [
    "TO DO LIST : \n",
    " - Ajout du tracé ---> OK\n",
    " - Ajout de data augmentation --->A DEBUGGUER\n",
    " - Ajout trace des callBacks dans DataFrame, pour savoir si un callback s'est enclenché (par urgent)\n",
    " - Ajout de prediction sur les modèles ayant eu un bon score en validation\n",
    "\n",
    "-  AJOUTER CETTE VARIABLE\n",
    "\"\"\"from tensorflow.keras.optimizers import Adam \n",
    "    adam = Adam(learning_rate=0.001)\"\"\"    \n",
    "           Chercher comment éviter l'affichage des WARNINGS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d346a8",
   "metadata": {},
   "source": [
    "## Import des librairies et fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcea6782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from joblib import load,dump\n",
    "import numpy as np\n",
    "\n",
    "#Keras\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# tensorlflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D,Dropout, Flatten, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import Callback,ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be64d0f",
   "metadata": {},
   "source": [
    "## Fonctions secondaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18d2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour récupérer le chemin absolu d'un fichier placé au même endroit que le notebook en cours\n",
    "def get_output_path_file(output_filename):\n",
    "    notebook_filename = os.path.abspath(\"__file__\")\n",
    "    notebook_directory = os.path.dirname(notebook_filename)\n",
    "    return os.path.join(notebook_directory, output_filename)\n",
    "\n",
    "# fonction pour récupérer le chemin absolu d'un dossier placé au même endroit que le notebook en cours\n",
    "def get_output_path_folder(output_foldername):\n",
    "    notebook_filename = os.path.abspath(\"__file__\")\n",
    "    notebook_directory = os.path.dirname(notebook_filename)\n",
    "    return os.path.join(notebook_directory, output_foldername)\n",
    "\n",
    "#fonction de generation des deux dataset train et validation\n",
    "def generation_train_validation(repertoire_train,image_size,batch_size,validation_split):\n",
    "    \n",
    "    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(     #création d'un itérateur qui va charger les images \n",
    "        repertoire_train,                                                     #lors des étapes d'entrainements\n",
    "        labels='inferred', \n",
    "        label_mode='categorical', \n",
    "        seed=123,  color_mode='rgb', \n",
    "        image_size=image_size ,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        subset='training')\n",
    "\n",
    "    validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        repertoire_train,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        color_mode='rgb',\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        validation_split=validation_split,\n",
    "        subset='validation',  \n",
    "        seed=123)\n",
    "  \n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "def generation_test(repertoire_test,image_size,batch_size):\n",
    "    test_dataset = tf.keras.preprocessing.image_dataset_from_directory(       \n",
    "        repertoire_test,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical', \n",
    "        seed=123,  color_mode='rgb', #Images à 3 canaux\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    return test_dataset\n",
    "\n",
    "# fonction de récupération des noms et shape de chaque couche du model\n",
    "def get_layer_info(model):\n",
    "    layer_info = []\n",
    "    for layer in model.layers:\n",
    "        layer_info.append({\n",
    "            \"Layer Name\": layer.name#,\n",
    "            #\"Output Shape\": layer.output_shape\n",
    "        })\n",
    "    return layer_info\n",
    "\n",
    "#Fonction pour tracer la courbe de suivi de loss et accuracy au fil des epochs\n",
    "def tracer_courbe_suivi(nom_df_de_suivi,numero_ligne_à_tracer):\n",
    "    \n",
    "    chemin_df = get_output_path_file(nom_df_de_suivi)\n",
    "    df = load(chemin_df)\n",
    "   \n",
    "    #Courbe d'entrainement\n",
    "    \n",
    "    epochs_range = range(1,len(df.loss[numero_ligne_à_tracer])+1,1)\n",
    "                         \n",
    "    loss = df.loss[numero_ligne_à_tracer]\n",
    "    accuracy = df.acc[numero_ligne_à_tracer]\n",
    "    val_loss = df.val_loss[numero_ligne_à_tracer]\n",
    "    val_accuracy = df.val_acc[numero_ligne_à_tracer]\n",
    "    \n",
    "    plt.figure(figsize = (8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xticks(range(1,len(df.loss[numero_ligne_à_tracer])+1,1))\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xticks(range(1,len(df.loss[numero_ligne_à_tracer])+1,1))\n",
    "    plt.title('Training and Validation Loss')\n",
    "    \n",
    "    return plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c19dc",
   "metadata": {},
   "source": [
    "## Fonction principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d902ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Définition des CallBacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\",\n",
    "                               min_delta = 0.01,\n",
    "                               patience = 3,\n",
    "                               verbose=1,\n",
    "                               restore_best_weights = True)\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                         patience = 2,\n",
    "                                         factor = 0.1,\n",
    "                                         cooldown = 4,\n",
    "                                         verbose=1,\n",
    "                                         min_lr = 1e-10)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath = get_output_path_folder(\"Fichiers_points_de_controles\"),\n",
    "                                    monitor = \"val_loss\",\n",
    "                                    verbose = 1,\n",
    "                                    save_best_only = False,\n",
    "                                    save_weights_only= False,\n",
    "                                    moder = \"auto\",\n",
    "                                    save_freq=\"epoch\")\n",
    "\n",
    "\n",
    "#Fonction de suivi de modèles de DL\n",
    "def test_DL_predict(repertoire_train,repertoire_test,image_size,batch_size,validation_split,liste_modele,optimizer,loss,\n",
    "                    metrics,epochs,chemin_df_score_DL,nom_fichier_controle):\n",
    "      \n",
    "    train_dataset,validation_dataset = generation_train_validation(repertoire_train,\n",
    "                                                                   image_size,batch_size,\n",
    "                                                                   validation_split) \n",
    "    test_dataset = generation_test(repertoire_test,image_size,batch_size)  #Ajout repertoire test\n",
    "\n",
    "    #nombre total d'échantillons dans l'ensemble de données d'entraînement\n",
    "    nb_img_train = tf.data.experimental.cardinality(train_dataset).numpy() # FL :On récupère une valeur de 1821 d'où le faible nombre d'étapes\n",
    "\n",
    "    #nombre total d'échantillons dans l'ensemble de données de validation\n",
    "    nb_img_test = tf.data.experimental.cardinality(validation_dataset).numpy()\n",
    "    \n",
    "    model_layers_info = []\n",
    "    for model in liste_modele:\n",
    "        layers_info = get_layer_info(model)\n",
    "        model_layers_info.append(layers_info)\n",
    "    if os.path.exists(chemin_df_score_DL):\n",
    "        df_import = load(chemin_df_score_DL)\n",
    "        print(\"récupération du df existant\")\n",
    "    else:\n",
    "        df_import = pd.DataFrame(columns=[\"image_size\",\"batch_size\",\"epochs\", \"optimizer\",\"fonction loss\", \"loss\",\"acc\", \"val_loss\",\"val_acc\", \"Duree_entrainement en sec\",\"test_acc\",\"Nom_fichier_contrôle\"])\n",
    "        print(\"création d'un dataframe\")\n",
    "    score = []\n",
    "    for i, model in enumerate(liste_modele):\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        print(f\"debut de l'entrainement du modèle {i}\")\n",
    "        debut = time.time()\n",
    "        # entrainement avec la méthode .fit()\n",
    "        history = model.fit(train_dataset, \n",
    "                        epochs = epochs,\n",
    "                        callbacks=[early_stopping, reduce_learning_rate,model_checkpoint],\n",
    "                        steps_per_epoch = nb_img_train//batch_size,\n",
    "                        validation_data = validation_dataset,\n",
    "                        validation_steps = nb_img_test//batch_size)\n",
    "        \n",
    "        print(f\"fin de l'entrainement du modèle {i}\")\n",
    "        fin = time.time()\n",
    "        duree_entrainement = fin - debut\n",
    "        layer_info = model_layers_info[i]\n",
    "        \n",
    "        # doc : https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "        \n",
    "        if np.max(history.history['val_acc']) >  0.4 :\n",
    "            score_test = model.evaluate(test_dataset,batch_size = batch_size, verbose='auto')\n",
    "            print(\"%s: %.2f%%\" % (model.metrics_names[0], score_test[0]*100))\n",
    "            print(\"%s: %.2f%%\" % (model.metrics_names[1], score_test[1]*100))\n",
    "\n",
    "        else:\n",
    "            score_test  = 'Non calculé'\n",
    "            print(\"le score de test n'a pas été calculé car le score d'entrainement est trop faible\")\n",
    "        \n",
    "        model.save(nom_fichier_controle + '.h5')\n",
    "        # Loads the weights\n",
    "               \n",
    "        model_scores = {\n",
    "            \"image_size\":f\"{image_size}\",\n",
    "            \"batch_size\": f\"{batch_size}\",\n",
    "            \"model\":f\"{layer_info}\",\n",
    "            \"epochs\": f\"{epochs}\",\n",
    "            \"optimizer\": f\"{optimizer}\",\n",
    "            \"fonction loss\": f\"{loss}\",\n",
    "            \"loss\": history.history['loss'],\n",
    "            \"acc\": history.history['acc'],\n",
    "            \"val_loss\":history.history['val_loss'],\n",
    "            \"val_acc\":history.history['val_acc'],\n",
    "            \"Duree_entrainement en sec\": duree_entrainement,\n",
    "            \"test_acc\": score_test,\n",
    "            \"Nom_fichier_contrôle\": f\"{nom_fichier_controle}.h5\"\n",
    "            \n",
    "        }\n",
    "            \n",
    "        score.append(model_scores)\n",
    "    df_score = pd.DataFrame(score)\n",
    "    df = pd.concat([df_import, df_score], ignore_index=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272bccbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
